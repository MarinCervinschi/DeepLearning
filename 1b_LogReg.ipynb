{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinCervinschi/DeepLearning/blob/main/1b_LogReg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFDsAyCeBg1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231a7c85-9c73-4124-f69d-ff1a317e5f19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d558934c490>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "import gdown\n",
        "#from googledrivedownloader import GoogleDriveDownloader\n",
        "#import GoogleDriveDownloader\n",
        "import zipfile\n",
        "\n",
        "eps_torch = torch.finfo(float).eps\n",
        "\n",
        "torch.manual_seed(191090)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core goals of the lab\n",
        "1) Learn how to use torch.optim instead of manual parameter updates\n",
        "2) Implement logistic regression (useful for classification)\n"
      ],
      "metadata": {
        "id": "p5-zINHEkkug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(f\"https://drive.google.com/uc?id=1SagLh5XNSV4znhlnkLRkV7zHPSDbOAqv\",\n",
        "               output=\"./got.zip\", quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(\"got.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "GSJ8YQ7SH7OQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268ff4e7-1f63-420d-94c8-3ccd755beafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SagLh5XNSV4znhlnkLRkV7zHPSDbOAqv\n",
            "To: /content/got.zip\n",
            "100%|██████████| 84.6k/84.6k [00:00<00:00, 57.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_got_dataset(path, train_split=0.8, verbose=True):\n",
        "    \"\"\"\n",
        "    Loads the Game of Thrones dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "        the relative path of the csv file.\n",
        "    train_split: float\n",
        "        percentage of training examples in [0, 1].\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        x_train: np.array\n",
        "            training characters. shape=(n_train_examples, n_features)\n",
        "        y_train: np.array\n",
        "            training labels. shape=(n_train_examples,)\n",
        "        train_names: np.array\n",
        "            training names. shape=(n_train_examples,)\n",
        "        x_test: np.array\n",
        "            test characters. shape=(n_test_examples, n_features)\n",
        "        y_test: np.array\n",
        "            test labels. shape=(n_test_examples,)\n",
        "        test_names: np.array\n",
        "            test names. shape=(n_test_examples,)\n",
        "        feature_names: np.array\n",
        "            an array explaining each feature. shape=(n_test_examples,)\n",
        "    \"\"\"\n",
        "\n",
        "    # read file into string ndarray\n",
        "    with open(path, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        data = np.array([row for row in reader])\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nLoaded dataset from {path}\")\n",
        "        print(f\"Shape: {data.shape[0]} rows × {data.shape[1]} columns\")\n",
        "\n",
        "        # print header\n",
        "        header = data[0]\n",
        "        print(\"Columns:\", \", \".join(header))\n",
        "\n",
        "        # print a preview of first 5 rows\n",
        "        print(\"\\nSample rows:\")\n",
        "        for row in data[1:6]:\n",
        "            print(\"  \", row)\n",
        "\n",
        "    # extract feature names\n",
        "    feature_names = data[0, 1:-1]\n",
        "\n",
        "    # shuffle data\n",
        "    data = data[1:]\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    # extract character names\n",
        "    character_names = data[:, 0]\n",
        "\n",
        "    # extract features X and targets Y\n",
        "    X = np.float32(data[:, 1:-1])\n",
        "    Y = np.float32(data[:, -1])\n",
        "\n",
        "    # normalize X\n",
        "    X -= np.min(X, axis=0)\n",
        "    X /= np.max(X, axis=0)\n",
        "\n",
        "    # add bias to X\n",
        "    X = np.concatenate((X, np.ones(shape=(X.shape[0], 1))), axis=1)\n",
        "    feature_names = np.concatenate((feature_names, np.array(['bias'])), axis=-1)\n",
        "\n",
        "    total_characters = X.shape[0]\n",
        "    test_sampling_probs = np.ones(shape=total_characters)\n",
        "    test_sampling_probs[Y == 1] /= float(np.sum(Y == 1))\n",
        "    test_sampling_probs[Y == 0] /= float(np.sum(Y == 0))\n",
        "    test_sampling_probs /= np.sum(test_sampling_probs)\n",
        "\n",
        "    # sample test people without replacement\n",
        "    n_test_characters = int(total_characters * (1 - train_split))\n",
        "    test_idx = np.random.choice(np.arange(0, total_characters), size=(n_test_characters,),\n",
        "                                replace=False, p=test_sampling_probs)\n",
        "    x_test = X[test_idx]\n",
        "    y_test = Y[test_idx]\n",
        "    test_names = character_names[test_idx]\n",
        "\n",
        "    # sample train people\n",
        "    train_sampling_probs = test_sampling_probs.copy()\n",
        "    train_sampling_probs[test_idx] = 0\n",
        "    train_sampling_probs /= np.sum(train_sampling_probs)\n",
        "\n",
        "    n_train_characters = int(total_characters * train_split)\n",
        "    train_idx = np.random.choice(np.arange(0, total_characters), size=(n_train_characters,),\n",
        "                                 replace=True, p=train_sampling_probs)\n",
        "    x_train = X[train_idx]\n",
        "    y_train = Y[train_idx]\n",
        "    train_names = character_names[train_idx]\n",
        "\n",
        "    return x_train, y_train, train_names, x_test, y_test, test_names, feature_names"
      ],
      "metadata": {
        "id": "ZVEt31u-BrDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, train_names, x_test, y_test, test_names, feature_names = load_got_dataset(path='got.csv', train_split=0.8)\n",
        "\n",
        "#convert from np_array to tensors\n",
        "x_train = torch.from_numpy(x_train).to(dtype=torch.float32)\n",
        "x_test = torch.from_numpy(x_test).to(dtype=torch.float32)\n",
        "y_train = torch.from_numpy(y_train).to(dtype=torch.float32)\n",
        "y_test = torch.from_numpy(y_test).to(dtype=torch.float32)"
      ],
      "metadata": {
        "id": "d_4R-fb8BrA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c84a49-5b40-49de-fe21-c6d7a8e3a6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded dataset from got.csv\n",
            "Shape: 1947 rows × 27 columns\n",
            "Columns: name, male, numDeadRelations, book1, book2, book3, book4, book5, bookCount, isMarried, isPopular, witnessed_wins, witnessed_losses, hadMoreWinsThanLosses, wasAttackerCommander, wasDefenderCommander, wasCommander, witnessed_own_attacker_size_mean, witnessed_opponent_attacker_size_mean, witnessed_own_defender_size_mean, witnessed_opponent_defender_size_mean, witnessed_major_deaths, witnessed_major_capture, battleCountAsAttackerCommander, battleCountAsDefenderCommander, battleCountAsCommander, isAlive\n",
            "\n",
            "Sample rows:\n",
            "   ['Viserys II Targaryen' '1' '11' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
            " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
            "   ['Walder Frey' '1' '1' '1' '1' '1' '1' '1' '5' '1' '1' '3' '0' '1' '1' '0'\n",
            " '1' '3166' '0' '0' '1166' '1' '2' '3' '0' '3' '1']\n",
            "   ['Addison Hill' '1' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0'\n",
            " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1']\n",
            "   ['Aemma Arryn' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0'\n",
            " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
            "   ['Sylva Santagar' '0' '0' '0' '0' '0' '1' '0' '1' '1' '0' '0' '0' '0' '0'\n",
            " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Element-wise sigmoid function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: torch.tensor\n",
        "        a torch tensor of any shape\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor\n",
        "        a tensor having the same shape of x.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Apply the sigmoid function on x.\n",
        "    See https://en.wikipedia.org/wiki/Sigmoid_function\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / (1 + torch.exp(x))"
      ],
      "metadata": {
        "id": "Jy4EGTv8Bq8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\" Models a logistic regression classifier. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor methd\"\"\"\n",
        "\n",
        "        #super().__init__()\n",
        "\n",
        "        # weights placeholder\n",
        "        self._w = None\n",
        "\n",
        "        # optimizer placeholder\n",
        "        self.optim = None\n",
        "\n",
        "        # loss placeholder\n",
        "        self.loss = None\n",
        "\n",
        "\n",
        "    def fit_sgd(self, X, Y, n_epochs, learning_rate, verbose=False):\n",
        "        \"\"\"\n",
        "        Implements the gradient descent training procedure.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: torch.tensor\n",
        "            data. shape=(n_examples, n_features)\n",
        "        Y: np.array\n",
        "            labels. shape=(n_examples,)\n",
        "        n_epochs: int\n",
        "            number of gradient updates.\n",
        "        learning_rate: float\n",
        "            step towards the descent.\n",
        "        verbose: bool\n",
        "            whether or not to print the value of cost function.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # weight initialization\n",
        "        self._w = torch.randn(n_features, requires_grad=True)\n",
        "\n",
        "        # optimizer initialization\n",
        "        self.optim = torch.optim.SGD([self._w], learning_rate)\n",
        "\n",
        "        # loss initialization\n",
        "        self.loss = torch.nn.BCELoss()\n",
        "\n",
        "        for e in range(n_epochs):\n",
        "\n",
        "            \"\"\"\n",
        "            # Empy optimizer gradient buffer\n",
        "            \"\"\"\n",
        "            self.optim.zero_grad()\n",
        "\n",
        "            \"\"\"\n",
        "            # Compute predictions\n",
        "            # -> preds = ...\n",
        "            \"\"\"\n",
        "            #preds = torch.sigmoid(X @ self._w)\n",
        "            preds = sigmoid(X @ self._w)\n",
        "\n",
        "            \"\"\"\n",
        "            # Print loss between Y and predictions p\n",
        "            # -> loss = ...\n",
        "            \"\"\"\n",
        "            #loss = self.loss(preds, Y)\n",
        "            loss = -(Y * torch.log(preds + eps_torch) + (1 - Y) * torch.log(1 - preds + eps_torch)).mean()\n",
        "\n",
        "            if verbose and e % 500 == 0:\n",
        "                print(f'Epoch {e:4d}: loss={loss}')\n",
        "\n",
        "            \"\"\"\n",
        "            # Gradient backpropagation\n",
        "            \"\"\"\n",
        "            loss.backward()\n",
        "\n",
        "            \"\"\"\n",
        "            # Parameters update\n",
        "            \"\"\"\n",
        "            self.optim.step()\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Function that predicts.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: torch.tensor\n",
        "            data to be predicted. shape=(n_test_examples, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        prediction: torch.tensor\n",
        "            prediction in {0, 1}.\n",
        "            Shape is (n_test_examples,)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Compute predictions.\n",
        "        a) compute the dot product between X and w\n",
        "        b) apply the sigmoid function (this way, y in [0,1])\n",
        "        c) discretize the output (this way, y in {0,1})\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          return torch.round(sigmoid(X @ self._w))"
      ],
      "metadata": {
        "id": "B0RWNLmKBq5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Main function\"\"\"\n",
        "\n",
        "logistic_reg = LogisticRegression()\n",
        "\n",
        "# train\n",
        "\n",
        "logistic_reg.fit_sgd(x_train, y_train, n_epochs=10000, learning_rate=0.01, verbose=True)\n",
        "\n",
        "# test\n",
        "predictions = logistic_reg.predict(x_test)\n",
        "\n",
        "accuracy = float(torch.sum(predictions == y_test)) / y_test.shape[0]\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "\n",
        "# plot_boundar(x_train, y_train, logistic_reg, title='Training Set')"
      ],
      "metadata": {
        "id": "9kJhGRq8Bq3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f55f70f-2684-4345-85da-374279e3178c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0: loss=1.1644622087478638\n",
            "Epoch  500: loss=0.8126418590545654\n",
            "Epoch 1000: loss=0.7340261340141296\n",
            "Epoch 1500: loss=0.6853218078613281\n",
            "Epoch 2000: loss=0.6546803712844849\n",
            "Epoch 2500: loss=0.6351860165596008\n",
            "Epoch 3000: loss=0.6225240230560303\n",
            "Epoch 3500: loss=0.6140925288200378\n",
            "Epoch 4000: loss=0.6083299517631531\n",
            "Epoch 4500: loss=0.6042888164520264\n",
            "Epoch 5000: loss=0.6013832688331604\n",
            "Epoch 5500: loss=0.5992438197135925\n",
            "Epoch 6000: loss=0.5976317524909973\n",
            "Epoch 6500: loss=0.5963902473449707\n",
            "Epoch 7000: loss=0.5954136252403259\n",
            "Epoch 7500: loss=0.5946295857429504\n",
            "Epoch 8000: loss=0.5939877033233643\n",
            "Epoch 8500: loss=0.5934524536132812\n",
            "Epoch 9000: loss=0.592998206615448\n",
            "Epoch 9500: loss=0.5926061272621155\n",
            "Test accuracy: 0.6452442159383034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nNdmzz9VnQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}